{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "CO2Uc94pAeFX",
      "metadata": {
        "id": "CO2Uc94pAeFX"
      },
      "source": [
        "# Relational Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gZMMSaUJ96Rh",
      "metadata": {
        "id": "gZMMSaUJ96Rh"
      },
      "source": [
        "## The Relational Database"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_zrzc-G2-Oo8",
      "metadata": {
        "id": "_zrzc-G2-Oo8"
      },
      "source": [
        "In this case we had a variable (<code>DbPath</code>) - containing the path of the database - and  actually two methods to implement \n",
        "* <code>getDbPath</code> to retrieve the path of the database, which returns a string \n",
        "* <code>setDbPath</code> to instead set up a new database path, which takes in input a string - the path - and returns a boolean (True in the case the path passed is empty, False if it isn't)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vBdOV7PU-S1P",
      "metadata": {
        "id": "vBdOV7PU-S1P"
      },
      "outputs": [],
      "source": [
        "class RelationalProcessor(object): \n",
        "    def __init__(self, dbPath=None):\n",
        "        self.dbPath=dbPath  \n",
        "    def getDbPath(self):\n",
        "        return self.dbPath\n",
        "    def setDbPath(self, path):\n",
        "        if path!='':\n",
        "            self.dpPath=path\n",
        "            return True\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-9Ci49H7-cOl",
      "metadata": {
        "id": "-9Ci49H7-cOl"
      },
      "source": [
        "We created these classes and methods basing ourself on the [data model](https://github.com/comp-data/2021-2022/tree/main/docs/project), which had been procured to us and basing ourselves on the lessons we attended.\n",
        "\n",
        "We defined the name of the class and then worked onto writing the method and most importantly the constructor, which will be called everytime a new object is built."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EE0C2V2497tm",
      "metadata": {
        "id": "EE0C2V2497tm"
      },
      "source": [
        "## The Relational Data Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wEGULrOr-kS2",
      "metadata": {
        "id": "wEGULrOr-kS2"
      },
      "source": [
        "Its single method is <code>uploadData</code>, which was used to insert the collection of data of the input path specified inside the database\n",
        "\n",
        "In this case we worked with two main data formats, which were CSV and JSON.\n",
        "\n",
        "* The first step in this case, was understanding the structure of these two different formats and of the data contained inside, always paying attention to the data model.\n",
        "* Once understood the structure, we moved onto understanding what information we could retrieve from the JSON and which from the CSV.\n",
        "* First of all we checked with an <i>if</i> whether the path in the input was empty or not.\n",
        "* Then we created another <i>if</i> node to check whether the data format passed was a CSV or a JSON, through the method <code>[endswith](https://www.w3schools.com/python/ref_string_endswith.asp)</code>.\n",
        "* Once we separated the data formats, we worked onto producing the different <code>DataFrames</code> that we'd need for the database, from the data formats. \n",
        "Since we'd be dealing with a relational database, in this case we worked with <q>a relational model of data, using tables of columns and row where each column is identified by a unique key</q>.\n",
        "* Through the method <code>[read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)</code> we created a dataframe out of the initial CSV and then worked onto cleaning it and renaming a few columns, ending up with a table (<i>Publications</i>) that had the following columns:\n",
        "    * <b>publicationInternalId</b>, which worked as an <i>InternalId</i> and was then extremely useful later for the questions.\n",
        "    * <b>doi</b> of the publication.\n",
        "    * <b>title</b>.\n",
        "    * <b>type</b>.\n",
        "    * <b>type</b>.\n",
        "    * <b>publicationYear</b>.\n",
        "    * <b>issue</b>, which characterized the subclass of <i>Journal Article</i>.\n",
        "    * <b>volume</b>, which characterized the subclass of <i>Journal Article</i>.\n",
        "    * <b>chapterNumber</b>, which characterized the subclass of <i>BookChapter</i>.\n",
        "    * <b>publicationVenue </b>, in this case not the relationship but the name of the venue, in which the publication was published.\n",
        "    * <b>venue_type</b>.\n",
        "    * <b>publisher</b>.\n",
        "    * <b>event</b>, which characterized the subclass of <i>Proceedings</i>.\n",
        "* After this we used the DataFrame method <i>to_sql</i> to add our table in the database.\n",
        "* But before this, we employed [SQLite](https://www.sqlite.org) to operate on the database, using the class <code>Connection</code>, to connect to a particular database, stored in a <i>db</i> file.\n",
        "This was obtained through the function <code>connect</code>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SHK2LpVf-m8x",
      "metadata": {
        "id": "SHK2LpVf-m8x"
      },
      "outputs": [],
      "source": [
        "class RelationalDataProcessor (RelationalProcessor):\n",
        "    def __init__(self, dbPath=None):\n",
        "        super().__init__(dbPath)\n",
        "    def uploadData (self, path):\n",
        "        import pandas as pd\n",
        "        from pandas import DataFrame\n",
        "        from sqlite3 import connect\n",
        "        from pandas import Series\n",
        "        if path !='':\n",
        "            if path.endswith(\".csv\"):\n",
        "                from pandas import read_csv\n",
        "            \n",
        "                df_rel_publications=read_csv (path,\n",
        "                                    keep_default_na=False,\n",
        "                                    encoding=\"utf-8\",\n",
        "                                    dtype={\n",
        "                                \"id\": \"string\",\n",
        "                                \"title\": \"string\",\n",
        "                                \"type\": \"string\",\n",
        "                                \"publication_year\": \"int\",\n",
        "                                \"issue\": \"string\",\n",
        "                                \"volume\": \"string\",\n",
        "                                \"chapter\": \"string\",\n",
        "                                \"publication_venue\": \"string\",\n",
        "                                \"venue_type\": \"string\",\n",
        "                                \"publisher\":\"string\",\n",
        "                                \"event\":\"string\"            \n",
        "                                }) \n",
        "                # Create a new column with internal identifiers for each publication\n",
        "                publication_internal_id = []\n",
        "                for idx, row in df_rel_publications.iterrows():\n",
        "                    publication_internal_id.append(\"publication-\" + str(row[\"id\"]))\n",
        "                df_rel_publications.insert(0, \"publicationInternalId\", Series(publication_internal_id, dtype=\"string\"))\n",
        "                publisher_internal_id = []\n",
        "                for idx, row in df_rel_publications.iterrows():\n",
        "                    publisher_internal_id.append (\"publisher-\" + str(row[\"publisher\"]))\n",
        "                del df_rel_publications[\"publisher\"]\n",
        "                df_rel_publications.insert(10, \"publisher\", Series(publisher_internal_id, dtype=\"string\"))\n",
        "\n",
        "                df_rel_publications= df_rel_publications.rename(columns={\"id\":\"doi\", \"publication_year\":\"publicationYear\", \n",
        "                \"chapter\":\"chapterNumber\", \"publication_venue\":\"publicationVenue\"})\n",
        "\n",
        "                with connect(self.dbPath) as con:\n",
        "                    df_rel_publications.to_sql(\"Publications\", con, if_exists=\"replace\", index=False)\n",
        "                    con.commit()\n",
        "                return True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZRtCtJMB-rsp",
      "metadata": {
        "id": "ZRtCtJMB-rsp"
      },
      "source": [
        "We then worked onto the data contained in the JSON file.\n",
        "\n",
        "* In this case we didn't immediately turn the JSON in a <code>DataFrame</code>, but instead after having loaded, we worked through accessing all its various sections, and only then we worked onto turning it in a <code>DataFrame</code>, through <code>[pd.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)</code>.\n",
        "* To further access to the data we needed we also used the <code>[iloc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html)</code> property to select the positions we needed, in the <code>DataFrames</code>.\n",
        "* Looking at the data, contained in the JSON, we soon realized that it was possible that the same publication might have more than one venue, author and citation, so to obviate to this problem, we created a single string out of the lists of venues and citations present inside of the JSON, since the lists themselves were unsupported by SQLite.\n",
        "* In this case we had 4 four main sections:\n",
        "    * <b>Authors</b>, which had the following columns:\n",
        "        * <b>authorInternalId</b>, created for the queries in the next step and containing the InternalId of the authors.\n",
        "        * <b>familyName</b>\n",
        "        * <b>givenName</b>\n",
        "        * <b> authorID</b>, the orcid.\n",
        "        * <b>publicationInternalId</b>, which would be later used as a key element for the queries.\n",
        "    * <b>Venues</b>, which had the following columns:\n",
        "        * <b>venueInternalId</b>, created for the queries in the next step and containing the InternalId of the venues.\n",
        "        * <b>venueID </b>, the venues ids.\n",
        "        * <b>publicationInternalId</b>, which would be later used as a key element for the queries.\n",
        "    * <b>Publishers</b>, which had the following columns:\n",
        "        * <b>publisherInternalId</b>, created for the queries in the next step and containing the InternalId of the venues.\n",
        "        * <b>organizationID</b>, the ids of the publishers.\n",
        "        * <b>name</b>, the names of the publishers.\n",
        "    * <b>Cited publications</b>.\n",
        "    In this case we created two tables, containing the same citations but in different ways:\n",
        "        * <b>CitedPublications</b> contained the rows:\n",
        "            * <b>id_all_references</b>, where all the citations were collected together in a single string, to avoid creating further problems in the generic stage.\n",
        "            * <b> publicationInternalId</b>, created for the queries in the next step and containing the InternalId of the venues.\n",
        "        * <b>CitedPublications1</b>, instead contained:\n",
        "            * <b>doi</b> of the publication.\n",
        "            * <b>CitedPublications</b>, with all the citations contained inside, but this time not linked together through a string, but instead for each different citation the doi is repeated inside the table, which created more rows, since some doi values were repeated.\n",
        "* After all the dataframes were created we ended up uploading them as well in the database, through the same methods as described above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DLQgvTfW-vwK",
      "metadata": {
        "id": "DLQgvTfW-vwK"
      },
      "outputs": [],
      "source": [
        "class RelationalDataProcessor (RelationalProcessor):\n",
        "    def __init__(self, dbPath=None):\n",
        "        super().__init__(dbPath)\n",
        "    def uploadData (self, path):\n",
        "        import pandas as pd\n",
        "        from pandas import DataFrame\n",
        "        from sqlite3 import connect\n",
        "        from pandas import Series\n",
        "        if path !='':\n",
        "            if path.endswith(\".csv\"):\n",
        "                from pandas import read_csv\n",
        "            \n",
        "                df_rel_publications=read_csv (path,\n",
        "                                    keep_default_na=False,\n",
        "                                    encoding=\"utf-8\",\n",
        "                                    dtype={\n",
        "                                \"id\": \"string\",\n",
        "                                \"title\": \"string\",\n",
        "                                \"type\": \"string\",\n",
        "                                \"publication_year\": \"int\",\n",
        "                                \"issue\": \"string\",\n",
        "                                \"volume\": \"string\",\n",
        "                                \"chapter\": \"string\",\n",
        "                                \"publication_venue\": \"string\",\n",
        "                                \"venue_type\": \"string\",\n",
        "                                \"publisher\":\"string\",\n",
        "                                \"event\":\"string\"            \n",
        "                                }) \n",
        "                # Create a new column with internal identifiers for each publication\n",
        "                publication_internal_id = []\n",
        "                for idx, row in df_rel_publications.iterrows():\n",
        "                    publication_internal_id.append(\"publication-\" + str(row[\"id\"]))\n",
        "                df_rel_publications.insert(0, \"publicationInternalId\", Series(publication_internal_id, dtype=\"string\"))\n",
        "                publisher_internal_id = []\n",
        "                for idx, row in df_rel_publications.iterrows():\n",
        "                    publisher_internal_id.append (\"publisher-\" + str(row[\"publisher\"]))\n",
        "                del df_rel_publications[\"publisher\"]\n",
        "                df_rel_publications.insert(10, \"publisher\", Series(publisher_internal_id, dtype=\"string\"))\n",
        "\n",
        "                df_rel_publications= df_rel_publications.rename(columns={\"id\":\"doi\", \"publication_year\":\"publicationYear\", \n",
        "                \"chapter\":\"chapterNumber\", \"publication_venue\":\"publicationVenue\"})\n",
        "\n",
        "                with connect(self.dbPath) as con:\n",
        "                    df_rel_publications.to_sql(\"Publications\", con, if_exists=\"replace\", index=False)\n",
        "                    con.commit()\n",
        "                return True\n",
        "\n",
        "            if path.endswith(\".json\"):\n",
        "                from json import load \n",
        "                with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    rel_data = load(f)\n",
        "\n",
        "                #authors' dataframe\n",
        "                findex=0\n",
        "                for x in rel_data[\"authors\"]:\n",
        "                    for y in rel_data[\"authors\"][x]:\n",
        "                        findex+=1\n",
        "\n",
        "                df_author=pd.DataFrame(columns=[\"doi\",\"family\",\"given\",\"orcid\"],index=range(findex))#dataframe with authors information\n",
        "                ind=0\n",
        "                for x in rel_data[\"authors\"]:\n",
        "                    for y in rel_data[\"authors\"][x]:\n",
        "                        df_author.iloc[ind] = (x,y[\"family\"],y[\"given\"],y[\"orcid\"])\n",
        "                        ind+=1\n",
        "                publication_internal_id = []\n",
        "                for idx, row in df_author.iterrows():\n",
        "                    publication_internal_id.append(\"publication-\" + str(row[\"doi\"]))\n",
        "                df_author.insert(4, \"publicationInternalId\", Series(publication_internal_id, dtype=\"string\"))\n",
        "                author_internal_id = []\n",
        "                for idx, row in df_author.iterrows():\n",
        "                    author_internal_id.append (\"author-\" + str(row[\"orcid\"]))\n",
        "                df_author.insert(0, \"authorInternalId\", Series(author_internal_id, dtype = \"string\"))\n",
        "                    \n",
        "                df_author = df_author[[\"authorInternalId\", \"family\", \"given\", \"orcid\", \"publicationInternalId\"]]\n",
        "                df_author= df_author.rename(columns={\"given\":\"givenName\", \"family\":\"familyName\",\"orcid\":\"authorID\"})\n",
        "\n",
        "                #dataframe with publications'doi and venues id\n",
        "                findex = 0\n",
        "                for x in rel_data[\"venues_id\"]:\n",
        "                    findex += 1\n",
        "                df_pub_venues = pd.DataFrame(columns = [\"doi\",\"venues_id\"], index= range(findex))\n",
        "                ind =0\n",
        "                for x in rel_data[\"venues_id\"]:\n",
        "                    df_pub_venues.iloc[ind] = (x, str(rel_data[\"venues_id\"][x]))\n",
        "                    ind +=1\n",
        "                publication_internal_id =[]\n",
        "                for idx, row in df_pub_venues.iterrows():\n",
        "                    publication_internal_id.append(\"publication-\" + str(row[\"doi\"]))\n",
        "                df_pub_venues.insert(0, \"publicationInternalId\", Series(publication_internal_id, dtype=\"string\"))\n",
        "                venue_internal_id= []\n",
        "                for idx, row in df_pub_venues.iterrows():\n",
        "                    venue_internal_id.append(\"venue-\" + str(idx))\n",
        "                df_pub_venues.insert(0, \"venueInternalId\", Series(venue_internal_id, dtype=\"string\"))\n",
        "\n",
        "                df_pub_venues = df_pub_venues[[\"venueInternalId\", \"venues_id\", \"publicationInternalId\"]]\n",
        "                df_pub_venues=df_pub_venues.rename(columns={\"venues_id\":\"venueID\"})\n",
        "\n",
        "                #dataframe for publishers \n",
        "                findex = 0\n",
        "                for x in rel_data[\"publishers\"]:\n",
        "                    findex+=1\n",
        "                df_publishers = pd.DataFrame(columns=[\"publisher_id\", \"name\"], index=range(findex))\n",
        "                ind=0 \n",
        "                for x in rel_data[\"publishers\"]:\n",
        "                    df_publishers.iloc[ind] = (rel_data[\"publishers\"][x][\"id\"], rel_data[\"publishers\"][x][\"name\"])\n",
        "                    ind +=1\n",
        "                publisher_internal_id = []\n",
        "                for idx, row in df_publishers.iterrows():\n",
        "                    publisher_internal_id.append (\"publisher-\" + str(row[\"publisher_id\"]))\n",
        "                df_publishers.insert(0, \"publisherInternalId\", Series(publisher_internal_id, dtype = \"string\"))\n",
        "                df_publishers=df_publishers.rename(columns={\"publisher_id\":\"organizationID\"})\n",
        "                \n",
        "                findex_references=0\n",
        "                for x in rel_data[\"references\"]:\n",
        "                    for y in rel_data[\"references\"][x]:\n",
        "                        findex_references +=1\n",
        "                cited_publications = pd.DataFrame(columns=[\"id\", \"id_references\"], index=range(findex_references))\n",
        "                ind = 0\n",
        "                for x in rel_data[\"references\"]:\n",
        "                    for y in rel_data[\"references\"][x]:\n",
        "                        cited_publications.iloc[ind]= (x, y)\n",
        "                        ind += 1\n",
        "                cited_publications = cited_publications.rename(columns={\"id\":\"doi\",\"id_references\":\"CitedPublications\"})\n",
        "            \n",
        "                #references \n",
        "                findex=0\n",
        "                for x in rel_data[\"references\"]:\n",
        "                    findex+=1\n",
        "                df_references = pd.DataFrame(columns=[\"id\", \"id_references\"], index=range(findex))\n",
        "                ind = 0\n",
        "                for x in rel_data[\"references\"]:\n",
        "                    df_references.iloc[ind]= (x, str(rel_data[\"references\"][x]))\n",
        "                    ind += 1        \n",
        "                        \n",
        "                for idx, row in df_references.iterrows():\n",
        "                    publication_internal_id.append(\"publication-\" + str(row[\"id\"]))\n",
        "                df_references.insert(2, \"publicationInternalId\", Series(publication_internal_id, dtype=\"string\"))\n",
        "                \n",
        "                df_references = df_references[[\"id_references\", \"publicationInternalId\"]]\n",
        "                df_references=df_references.rename(columns={\"id_references\":\"id_all_references\"})\n",
        "\n",
        "                # Cites\n",
        "            \n",
        "            \n",
        "                with connect(self.dbPath) as con:\n",
        "                    \n",
        "                    df_author.to_sql(\"Authors\", con, if_exists=\"replace\", index=False)\n",
        "                    df_publishers.to_sql(\"Publishers\", con, if_exists=\"replace\", index=False)\n",
        "                    df_pub_venues.to_sql(\"Venues\", con, if_exists=\"replace\", index=False)\n",
        "                    df_references.to_sql(\"CitedPublications\", con, if_exists=\"replace\", index=False)\n",
        "                    cited_publications.to_sql(\"CitedPublications1\", con, if_exists=\"replace\", index=False)\n",
        "                    con.commit()\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S_u8nP4s_KNc",
      "metadata": {
        "id": "S_u8nP4s_KNc"
      },
      "source": [
        "## The Relational Query Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oaThPZz7_U_f",
      "metadata": {
        "id": "oaThPZz7_U_f"
      },
      "source": [
        "Now that we had effectively uploaded our data in our relational database, the next step was to retrieve the specific dataframes that we'd then need to actually implement the methods required.\n",
        "\n",
        "To do so, we used the <b>[SQL](https://it.wikipedia.org/wiki/Structured_Query_Language)</b> (Structure Query Language) to further manage our database and retreieve the specific results we needed.\n",
        "\n",
        "The first method that we had to implement was the <code>getPublicationsPublishedInYear</code>, which takes in input a number (the publication year, in this case) and returns a dataframe with all the publications published in that specific year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CRv8SZXL_eKD",
      "metadata": {
        "id": "CRv8SZXL_eKD"
      },
      "outputs": [],
      "source": [
        "from pandas import merge \n",
        "from pandas import Series\n",
        "from pandas import DataFrame\n",
        "from sqlite3 import connect\n",
        "from pandas import read_sql\n",
        "from pandas import read_csv\n",
        "from sqlite3 import connect\n",
        "\n",
        "class RelationalQueryProcessor (RelationalProcessor):\n",
        "    def __init__(self, dbpath):\n",
        "        super().__init__(dbpath)\n",
        "\n",
        "\n",
        "    def getPublicationsPublishedInYear(self, input_year):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query = \"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "            Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "            GROUP_CONCAT(Authors.authorID)\n",
        "            FROM Publications\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE Publications.publicationYear = '{0}'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(input_year)\n",
        "            publication_year_df = read_sql(query, con)\n",
        "            publication_year_df = publication_year_df.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "        return publication_year_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zzO6QMF6_e_J",
      "metadata": {
        "id": "zzO6QMF6_e_J"
      },
      "source": [
        "First of all, we connected to the database through the <code>with</code> clause but worked offline, to avoid any unexpected behaviors.\n",
        "\n",
        "Following this we also needed the <code>[read_sql](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)</code> method, which enables us to use SQL on a SQL-based-database; it takes into consideration two parameters: \n",
        "* the query executed on the database (<code>query</code>).\n",
        "* the connection to said database (<code>con</code>).\n",
        "\n",
        "As for the part of the query, since we need ro retrieve a <b>Publication</b>, we chose in the <code>SELECT</code> statement <i>Publications.publicationInternalId</i>, <i>Publications.doi</i>, <i>Publications.title</i>, <i>Publications.publicationYear</i>, <i>Venues.venueID</i> (to express the relation <i>publicationVenue</i>), <i>CitedPublications.id_all_references</i>(to express the relation <i>cites</i>), <i>GROUP_CONCAT(Authors.authorID)</i>(to express the relation <i>author</i>), since they are the basic element characterizing such a class.\n",
        "\n",
        "To retrieve all of these columns, we started <code>FROM</code> Publication and then through a series of <code>LEFT JOIN</code> we manage to join together the tables we need to fully return the <code>DataFrame</code> containing all the publication's essentials columns.\n",
        "\n",
        "Then we come to the <code>WHERE</code> statement, where we specify the condition: in this case the Publications.publicationYear, needs to be same as the one stated by the input, which is obtained through the employment of the <code>[.format](\"https://www.w3schools.com/python/ref_string_format.asp\")</code> method.\n",
        "\n",
        "This method formats the specificified value and insert it inside of the string's placeholder.\n",
        "\n",
        "We used a similar method for the following questions which required to retrieve a <code>DataFrame</code> with all the publications, adapting our queries to the need of the method required.\n",
        "\n",
        "For example in the case of the following queries, <code>getPublicationsByAuthorId</code>, have in the same rows, all the authors that collaborated to that publication, we created a second query, from which we retrieved a DataFrame with the columns, <i>GROUP_CONCAT(Authors.authorID)</i> and the <i>Publications.publicationInternalId</i>.\n",
        "\n",
        "We then merged this <code>DataFrame</code> with the one we had obtained from the previous query, to make all the authors of the publication appear, instead of just the one we used in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MLDlqN1h_iOX",
      "metadata": {
        "id": "MLDlqN1h_iOX"
      },
      "outputs": [],
      "source": [
        "def getPublicationsByAuthorId(self,authorID):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query =  \"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "            Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references\n",
        "            FROM Publications\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE Authors.authorID = '{0}'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(authorID)\n",
        "            publication_author_df = read_sql(query, con)\n",
        "            \n",
        "            query_2=\"\"\"SELECT  GROUP_CONCAT(Authors.authorID), Publications.publicationInternalId\n",
        "            FROM PUBLICATIONS\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\"\n",
        "            all_authors= read_sql(query_2, con)\n",
        "            all_authors= all_authors.rename(columns={\"GROUP_CONCAT(Authors.authorID)\":\"all_authors_id\"})\n",
        "            \n",
        "            publication_all_authors = publication_author_df.merge(all_authors)\n",
        "            \n",
        "        return publication_all_authors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MqqRlwdy_kKf",
      "metadata": {
        "id": "MqqRlwdy_kKf"
      },
      "source": [
        "We then had to work with the two methods <code>getMostCitedPublication</code> and <code>getMostCitedVenue</code>, which retrieve respectively:\n",
        "* the DataFrame with all the publications that received the most number of citations.\n",
        "* the Dataframe with all the venues containing the publications that received the most number of citations.\n",
        "\n",
        "In this case we set to retrieve - for the first query - the same columns as we did for the previous questions, but in the FROM statement, we nestled another query, to set out to find inside of the CitedPublications1 DataFrame, which was the most cited publications.\n",
        "\n",
        "To do so we used <code>COUNT</code> to count all the <i>CitedPublications1.citedPublications</i>, and store the name under the name <i>Cited</i> (which was added inside of the SELECT statement), we then grouped them all together through <code>GROUP BY</code> and then ordered them in a descendant order (<code>DESC</code>), setting the <code>LIMIT</code> at 1, this way we'd retrieve only the first result, which would be the one who had received the most citations.\n",
        "\n",
        "After closing this nestled query, we went back to work on the original one, using the <code>LEFT JOIN</code> to retrieve all the columns which were missing from our starting point.\n",
        "\n",
        "We had a similar process for the implementation for the getMostCitedVenue, but this time in the <code>SELECT</code> statement, we used all the columns that we associated to the Venue class (plus <i>Cited</i>):  <i>Venues.venueInternalId</i>, <i>Publications.publicationVenue</i>, <i>Venues.venueID</i>, <i>Publishers.organizationID</i>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XgwWlLqf_rby",
      "metadata": {
        "id": "XgwWlLqf_rby"
      },
      "outputs": [],
      "source": [
        "def getMostCitedPublication(self):\n",
        "    with connect(self.dbPath) as con:\n",
        "        query =\"\"\"SELECT DISTINCT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "        Publications.publicationYear, Venues.venueID, GROUP_CONCAT(Authors.authorID), Cited\n",
        "        FROM (SELECT CitedPublications1.\"citedPublications\", COUNT (CitedPublications1.\"citedPublications\") as Cited\n",
        "        FROM CitedPublications1\n",
        "        GROUP BY CitedPublications1.\"citedPublications\"\n",
        "        ORDER BY Cited DESC\n",
        "        LIMIT 1)\n",
        "        LEFT JOIN Publications ON Publications.doi==\"citedPublications\"\n",
        "        LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "        GROUP BY Publications.publicationInternalId;\"\"\"\n",
        "        mostcitedpub=read_sql(query, con)\n",
        "        mostcitedpub= mostcitedpub.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "           \n",
        "    return mostcitedpub\n",
        "def getMostCitedVenue(self):\n",
        "    with connect(self.dbPath) as con:\n",
        "        query =\"\"\"SELECT DISTINCT Venues.venueInternalId, Publications.publicationVenue, Venues.venueID, Publishers.organizationID,  Cited\n",
        "        FROM (SELECT CitedPublications1.\"citedPublications\", COUNT (CitedPublications1.\"citedPublications\") as Cited\n",
        "        FROM CitedPublications1\n",
        "        GROUP BY CitedPublications1.\"citedPublications\"\n",
        "        ORDER BY Cited DESC\n",
        "        LIMIT 1)\n",
        "        LEFT JOIN Publications ON Publications.doi==\"citedPublications\"\n",
        "        LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Publishers ON Publishers.publisherInternalId==Publications.publisher\n",
        "        GROUP BY Publications.publicationInternalId;\"\"\"\n",
        "        mostcitedvenue=read_sql(query, con)\n",
        "        mostcitedvenue= mostcitedvenue.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\", \"publicationVenue\":\"title\"})\n",
        "    return mostcitedvenue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "alwk0dAQ_tj1",
      "metadata": {
        "id": "alwk0dAQ_tj1"
      },
      "source": [
        "Following these questions, we went back to the usual system, using the <code>SELECT</code> statement for the class Venue for the method <code>getVenuesbyPublisherId</code>, and the <code>LEFT JOIN</code> option, to obtain all the columns we needed to execute the queries.\n",
        "\n",
        "We used the same logic for also the method <code>getPublicationInVenue</code>, but retrieving the columns for the class Publication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BA-Vdfo-_vid",
      "metadata": {
        "id": "BA-Vdfo-_vid"
      },
      "outputs": [],
      "source": [
        "def getVenuesByPublisherId(self,publisherid):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query=\"\"\"SELECT Venues.venueInternalId, Publications.publicationVenue, Venues.venueID, Publishers.organizationID\n",
        "            FROM Publications\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId \n",
        "            LEFT JOIN Publishers ON Publishers.publisherInternalId==Publications.publisher\n",
        "            WHERE Publications.publisher LIKE '%{0}%'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(publisherid)\n",
        "            venbypub=read_sql(query, con)\n",
        "            venbypub=venbypub.rename(columns={\"publicationVenue\":\"title\"})\n",
        "        return venbypub\n",
        "def getPublicationInVenue(self,venueid):\n",
        "    with connect(self.dbPath) as con:\n",
        "        query =\"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "        Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "        GROUP_CONCAT(Authors.authorID)\n",
        "        FROM Publications\n",
        "        LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "        WHERE Venues.venueID LIKE '%{0}%'\n",
        "        GROUP BY Publications.doi;\"\"\".format(venueid)\n",
        "        pubinvenue = read_sql(query, con)\n",
        "        pubinvenue= pubinvenue.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "    return pubinvenue"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fp_Vgfb4_xrN",
      "metadata": {
        "id": "Fp_Vgfb4_xrN"
      },
      "source": [
        "For the following three methods, we had to use a similar approach.\n",
        "\n",
        "For <code>getJournalArticleInIssue</code>, <code>getJournalArticleInVolume</code>, <code>getJournalArticleinJournal</code> we actually all had to retrieve the same columns: <i>Publications.publicationInternalId, Publications.doi, Publications.title,Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, GROUP_CONCAT(Authors.authorID), Publications.issue, Publications.volume</i>.\n",
        "\n",
        "Basically we were working with the same columns as when working with a publication, but we had to keep in mind that the subclass <i>JournalArticle</i> had also volume and issue as attributes, so we added them in the <code>SELECT</code>.\n",
        "\n",
        "What changed through these three different methods was the number of the input parameters, which we solved through using the usual <code>format</code> method and placeholders.\n",
        "\n",
        "In the first method we had actually retrieve the specific <i>JournalArticle</i>, which had the specific volume, issue and venue id explicited in the parameter, whereas in the second we had in input just the volume and venue id and in the last one just the venue id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Syk3kWa-_zxc",
      "metadata": {
        "id": "Syk3kWa-_zxc"
      },
      "outputs": [],
      "source": [
        "def getJournalArticlesInIssue(self,issue, volume,journal_id):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query =\"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "            Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "            GROUP_CONCAT(Authors.authorID), Publications.issue, Publications.volume\n",
        "            FROM Publications\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE  Publications.issue = '{0}' AND Publications.volume = '{1}' AND Venues.venueID LIKE '%{2}%'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(issue,volume,journal_id)\n",
        "            journalissue = read_sql(query, con)\n",
        "            journalissue= journalissue.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "        return journalissue\n",
        "def getJournalArticlesInVolume(self,volume,journal_id):\n",
        "    with connect(self.dbPath) as con:\n",
        "        query =\"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "        Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "        GROUP_CONCAT(Authors.authorID), Publications.issue, Publications.volume\n",
        "        FROM Publications\n",
        "        LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "        WHERE Publications.volume = '{0}' AND Venues.venueID LIKE '%{1}%'\n",
        "        GROUP BY Publications.publicationInternalId;\"\"\".format(volume,journal_id)\n",
        "        journalvolume = read_sql(query, con)\n",
        "        journalvolume= journalvolume.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "    return journalvolume\n",
        "def getJournalArticlesInJournal(self,journal_id):\n",
        "    with connect(self.dbPath) as con:\n",
        "        query =\"\"\"SELECT Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "        Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "        GROUP_CONCAT(Authors.authorID), Publications.issue, Publications.volume\n",
        "        FROM Publications\n",
        "        LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "        LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "        WHERE Venues.venueID LIKE '%{0}%'\n",
        "        GROUP BY Publications.publicationInternalId;\"\"\".format(journal_id)\n",
        "        journaljournal = read_sql(query, con)\n",
        "        journaljournal= journaljournal.rename(columns={\"GROUP_CONCAT(Authors.authorID)\": \"all_authors_id\"})\n",
        "    return journaljournal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ZgVpOdv_2lw",
      "metadata": {
        "id": "1ZgVpOdv_2lw"
      },
      "source": [
        "One thing that we actually had to take in consideration with the following method - <code>getProceedingsByEvent</code> - and be careful about was the fact that the event in input could be just a <b>partial match</b>, so, in the <i>WHERE</i> condition, we added both the <i>[wildcards](https://www.w3schools.com/sql/sql_wildcards.asp)</i> character <code>%</code>, around the placeholders (which basically mean zero or more character before or after what we pass as input) and the <code>[LIKE](https://www.w3schools.com/sql/sql_like.asp)</code> operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bx1MOG6u_4uE",
      "metadata": {
        "id": "bx1MOG6u_4uE"
      },
      "outputs": [],
      "source": [
        "def getProceedingsByEvent(self,event):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query =\"\"\"SELECT Venues.venueInternalId, Publications.publicationVenue, Venues.venueID, Publications.publisher, Publications.event\n",
        "            FROM Publications\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE Publications.event LIKE '%{0}%'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(event)\n",
        "            procevent= read_sql(query, con)\n",
        "            procevent=procevent.rename(columns={\"publicationVenue\":\"title\", \"publisher\":\"organizationID\"})\n",
        "        return procevent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IUmRF0c4_-sE",
      "metadata": {
        "id": "IUmRF0c4_-sE"
      },
      "source": [
        "For the method <code>getPublicationAuthors</code>, we used the same method as always but selecting the columns which characterizes the class <i>Author</i> (<i>Authors.familyName, Authors.givenName, Authors.authorID</i>).\n",
        "\n",
        "We worked <code>FROM</code> the table <i>Authors</i> and joined it with the one of <i>Publications</i> to then execute the <code>WHERE</code> condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BjrsZ_PL__I0",
      "metadata": {
        "id": "BjrsZ_PL__I0"
      },
      "outputs": [],
      "source": [
        "def getPublicationAuthors(self,publid):\n",
        "        with connect(self.dbPath) as con:\n",
        "            query =\"\"\"SELECT Authors.familyName, Authors.givenName, Authors.authorID\n",
        "            FROM Authors\n",
        "            LEFT JOIN Publications ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE Publications.doi == '{0}';\"\"\".format(publid)\n",
        "            publidf= read_sql(query, con)\n",
        "        return publidf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AmBb47VsAAd6",
      "metadata": {
        "id": "AmBb47VsAAd6"
      },
      "source": [
        "For the following method, <code>getPublicationsByAuthorName</code> we actually brought together all the logic we had learned through the previous queries, since also in this case we had to use the <code>LIKE</code> operator and the <code>%</code> since we'd be dealing with a potential partial match in input.\n",
        "\n",
        "We also used the same trick as for the code <code>getPublicationsByAuthorId</code> to obtain all the authors of the same publication in one row, merging a <code>DataFrame</code> obtained through a second query.\n",
        "\n",
        "Finally we added the the concatenation operator <code>||</code> to concatenate together the family and given name, because we found that through trying the code, the original idea we had would work only if passed in input the family or the given name of the author (even just a partial match).\n",
        "\n",
        "We wanted to actually make sure that also through inserting the full name (given and family, together) of the author (lowercase or uppercase and with a space between or not), it'd be returning all the results properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_Vwycl8ACbN",
      "metadata": {
        "id": "2_Vwycl8ACbN"
      },
      "outputs": [],
      "source": [
        "def getPublicationsByAuthorName(self,authorname):\n",
        "        import re\n",
        "        with connect(self.dbPath) as con:\n",
        "            query =\"\"\"SELECT Authors.familyName,\n",
        "            Publications.publicationInternalId, Publications.doi, Publications.title, \n",
        "            Publications.publicationYear, Venues.venueID, CitedPublications.id_all_references, \n",
        "            GROUP_CONCAT(Authors.authorID), Publications.issue, Publications.volume\n",
        "            FROM Publications\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN Venues ON Venues.publicationInternalId == Publications.publicationInternalId\n",
        "            LEFT JOIN CitedPublications ON CitedPublications.publicationInternalId == Publications.publicationInternalId\n",
        "            WHERE \n",
        "            Authors.givenName ||''|| Authors.familyName LIKE '%{0}%' OR\n",
        "            Authors.givenName ||' '|| Authors.familyName LIKE '%{0}%' OR\n",
        "            Authors.familyName ||''|| Authors.givenName LIKE '%{0}%' OR\n",
        "            Authors.familyName ||' '|| Authors.givenName LIKE '%{0}%'\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\".format(authorname)\n",
        "\n",
        "            authornamedf = read_sql(query, con)\n",
        "            \n",
        "          \n",
        "            \n",
        "            query_2=\"\"\"SELECT  GROUP_CONCAT(Authors.authorID), Publications.publicationInternalId\n",
        "            FROM PUBLICATIONS\n",
        "            LEFT JOIN Authors ON Authors.publicationInternalId == Publications.publicationInternalId\n",
        "            GROUP BY Publications.publicationInternalId;\"\"\"\n",
        "            all_authors= read_sql(query_2, con)\n",
        "            all_authors= all_authors.rename(columns={\"GROUP_CONCAT(Authors.authorID)\":\"all_authors_id\"})\n",
        "            \n",
        "            publications_by_author_name_df=authornamedf.merge(all_authors)\n",
        "        return  publications_by_author_name_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C4x0zyIcAFP7",
      "metadata": {
        "id": "C4x0zyIcAFP7"
      },
      "source": [
        "The last question resulted a bit tricky, since we'd have to return a DataFrame with all the <i>distinct</i> publishers that have published the publications specified in input (as a list).\n",
        "\n",
        "To obtain this we actually created another <code>DataFrame</code> (at the moment empty), and then through a for iterated all the elements present in the input list.\n",
        "\n",
        "* First we executed a normal query to obtain what we needed with the first element iterated, and as always stored it in a variable through <code>read_sql</code>, then we actually concatenated this <code>DataFrame</code>, with the empty one we created at the beginning.\n",
        "* We then went back to the beginning with another element of the input list, and this time the empty <code>DataFrame</code> we had at the beginning would be merged with the new one, effectively.\n",
        "\n",
        "This way we'd return the distinct publishers, as wanted by the method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDCZ3sgxAG4E",
      "metadata": {
        "id": "bDCZ3sgxAG4E"
      },
      "outputs": [],
      "source": [
        "def getDistinctPublisherOfPublications(self, plist):\n",
        "        import pandas as pd\n",
        "        from pandas import DataFrame\n",
        "        from pandas import concat\n",
        "        with connect(self.dbPath) as con:\n",
        "            pubId=DataFrame()\n",
        "            for el in plist:\n",
        "                query=\"\"\"SELECT Publishers.organizationID, Publishers.name\n",
        "                FROM Publishers\n",
        "                LEFT JOIN Publications ON Publications.publisher==Publishers.publisherInternalId\n",
        "                WHERE Publications.doi ='{0}';\"\"\".format(el)\n",
        "                distinctpub_df=read_sql(query, con)\n",
        "                pubId=pd.concat([pubId, distinctpub_df])\n",
        "            return pubId"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5jpmJhnqBhPU",
      "metadata": {
        "id": "5jpmJhnqBhPU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4KGleFaCAyWH",
      "metadata": {
        "id": "4KGleFaCAyWH"
      },
      "source": [
        "# Triplestore Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a3c266-d46c-46cf-a7f5-27e64b7f11a2",
      "metadata": {
        "id": "60a3c266-d46c-46cf-a7f5-27e64b7f11a2"
      },
      "source": [
        "## The Graph Database\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291db441-4558-4f3c-b424-e18ff678f77d",
      "metadata": {
        "id": "291db441-4558-4f3c-b424-e18ff678f77d"
      },
      "source": [
        "Our goal was that of translating such a data model into an RDF graph database:\n",
        "![image.png](attachment:55bd63c9-6721-452b-aec3-905deb8d53e3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72be095-022a-4e15-b360-2d1b093f29e1",
      "metadata": {
        "id": "a72be095-022a-4e15-b360-2d1b093f29e1"
      },
      "source": [
        "First of all, we have identified the names of all the most concrete classes (e.g. <b>JournalArticle</b>, <b>BookChapter</b>, <b>ProceedingsPaper</b>,<b> Journal</b>,<b> Book</b>,<b> Proceedings</b>,<b> Person</b>, <b>Organization</b>). </br>\n",
        "Then we established that each attribute of each UML class would have been represented by a distinct RDF property which would have been involved in statements where the subjects are always resources of the class in consideration and the objects are simple literals (i.e. values). Of course, we had to identify the names of these properties (i.e. the URLs).</br>\n",
        "We also decided that each relation starting from an UML class and ending in another UML class would have been represented by a distinct RDF property which would have been involved in statements where the subjects are always resources of the source class while the objects are resources of the target class. As well as said before, we had to identify the names of these properties (i.e. the URLs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f6399b-f56b-429e-b654-b04fa836309f",
      "metadata": {
        "id": "32f6399b-f56b-429e-b654-b04fa836309f"
      },
      "source": [
        "For creating the graph database, we have used the method `uploadData` of the class `TriplestoreQueryProcessor`.\n",
        " This method takes in input the path of the file you have to upload and returns a boolean which confirmes or not the effective success of the operation. <br>We have implemented this method starting from the creation of an empty `Graph` and of the `URIRef` for all the classes, attributes and relations among classes in our data model. For creating them, we have used the existing standards of [Schema.org](https://schema.org/). <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffdf2eb-3037-45ef-bf9b-a0c2dabf948d",
      "metadata": {
        "id": "2ffdf2eb-3037-45ef-bf9b-a0c2dabf948d"
      },
      "outputs": [],
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "my_graph = Graph()\n",
        "\n",
        "from rdflib import URIRef\n",
        "#classes of resources \n",
        "Person= URIRef(\"https://schema.org/Person\")\n",
        "JournalArticle= URIRef (\"https://schema.org/ScholarlyArticle\")\n",
        "BookChapter=URIRef (\"https://schema.org/Chapter\")\n",
        "Journal= URIRef(\"https://schema.org/Periodical\")\n",
        "Book= URIRef(\"https://schema.org/Book\")\n",
        "Organization= URIRef(\"https://schema.org/Organization\")\n",
        "ProceedingsPaper= URIRef(\"https://schema.org/Article\")\n",
        "Proceedings= URIRef(\"https://schema.org/Event\")\n",
        "\n",
        "#attributes related to classes\n",
        "givenName = URIRef(\"https://schema.org/givenName\")\n",
        "familyName = URIRef(\"https://schema.org/familyName\")\n",
        "id = URIRef(\"https://schema.org/identifier\")\n",
        "publicationYear = URIRef(\"https://schema.org/datePublished\")\n",
        "title = URIRef(\"https://schema.org/name\")\n",
        "issue = URIRef(\"https://schema.org/issueNumber\")\n",
        "volume = URIRef(\"https://schema.org/volumeNumber\")\n",
        "name = URIRef(\"https://schema.org/name\")\n",
        "chapterNumber = URIRef(\"https://schema.org/Number\")\n",
        "event = URIRef(\"https://schema.org/releasedEvent\")\n",
        "n_citations = URIRef(\"http://purl.org/spar/cito/isCitedBy\")\n",
        "\n",
        "#relations among classes\n",
        "publicationVenue = URIRef(\"https://schema.org/isPartOf\")\n",
        "publisher= URIRef(\"https://schema.org/publisher\")\n",
        "cites = URIRef (\"https://schema.org/citation\")\n",
        "author = URIRef(\"https://schema.org/author\")\n",
        "creators = URIRef (\"https://schema.org/creator\")\n",
        "all_citations = URIRef (\"https://schema.org/relatedLink\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9e3f95-5d65-4b94-b3b6-294414af23e0",
      "metadata": {
        "id": "ba9e3f95-5d65-4b94-b3b6-294414af23e0"
      },
      "source": [
        "Then, we created our `base_url`, in order to use it as a starting point for generating the URI identifying our resources: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f93c9b5-4e70-4a06-b41d-f3f1fba69421",
      "metadata": {
        "id": "1f93c9b5-4e70-4a06-b41d-f3f1fba69421"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://my_little_py.github-io/res/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "619f368f-e719-4a73-bcda-8c7aa1bb3031",
      "metadata": {
        "id": "619f368f-e719-4a73-bcda-8c7aa1bb3031"
      },
      "source": [
        "Then, we had to upload our data on the database starting from two files: a CSV containing mostly the data regarding the attributes of the class `Publication` and a JSON containing the information about the `Authors`, `Venues`, `Publishers` and `References` to other publications.<br> We had to put these data on the database separately, in a way that , independently from the file format we were loading, we would have obtained a graph database.\n",
        "So, we have checked the format of the files we had to upload: in order to distinguish between a JSON and a CSV, we have used an <b>if</b> statement. <br>After doing that, we wrote the code for loading our data on the database according to the file format they are stored into and to the structure of the JSON and CSV files we had to load."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c202d34a-805d-4d4e-8321-fad09808e8c2",
      "metadata": {
        "id": "c202d34a-805d-4d4e-8321-fad09808e8c2"
      },
      "source": [
        "In order to upload the data contained in the CSV file, we had first to open the CSV file. To do this, we used the function `read_csv` from Pandas, that takes in input a file path and returns a `DataFrame` representing such tabular data. </br>\n",
        "We have also specifyed an additional input named parameter, i.e. `dtype`, which enables the specification of a dictionary where the keys are column names, while the values are the strings representing the data type for each column. Instead, to force Pandas to use an empty string as default for string-based column in case of missing values, it is enough to tell the function `read_csv` not to use the default `NaN` for missing value by setting the input named parameter `keep_default_na` to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b457b207-6d47-4a03-9480-46c0b72f017d",
      "metadata": {
        "id": "b457b207-6d47-4a03-9480-46c0b72f017d"
      },
      "outputs": [],
      "source": [
        "if path.endswith(\".csv\"):\n",
        "    from pandas import read_csv\n",
        "    df_graph_publications = read_csv (path,\n",
        "                          keep_default_na=False,\n",
        "                          encoding=\"utf-8\",\n",
        "                        dtype={\n",
        "                       \"id\": \"string\",\n",
        "                       \"title\": \"string\",\n",
        "                       \"type\": \"string\",\n",
        "                       \"publication year\": \"int\",\n",
        "                       \"issue\": \"string\",\n",
        "                       \"volume\": \"string\",\n",
        "                       \"chapter\": \"string\",\n",
        "                       \"publication venue\": \"string\",\n",
        "                       \"venue_type\": \"string\",\n",
        "                       \"publisher\":\"string\",\n",
        "                       \"event\":\"string\"            \n",
        "                    }) \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "355e6ffd-90c8-46d9-b920-672aa656c00f",
      "metadata": {
        "id": "355e6ffd-90c8-46d9-b920-672aa656c00f"
      },
      "source": [
        "The next step was that of adding data to our graph database in the form of <b>RDF statements</b>. Each statement is a triple subject-predicate-object, where the subject is a resource, the predicate is a property, and the object is either a resource or a literal (i.e. a string). </br>\n",
        "We needed to use the method `add` to add a new RDF statement to a graph. Such method takes in input a tuple of three elements defining the subject (an `URIRef`), the predicate (another `URIRef`) and the object (either an `URIRef` or a `Literal`) of the statements.\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825de008-b2d5-4297-9091-530b36990a75",
      "metadata": {
        "id": "825de008-b2d5-4297-9091-530b36990a75"
      },
      "source": [
        "The following code shows how we populated the RDF graph using the data obtained by processing the CSV document. For instance, all the venues have been created using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66f6b6e-e62e-4990-9780-a4ba2b5ef95b",
      "metadata": {
        "id": "a66f6b6e-e62e-4990-9780-a4ba2b5ef95b"
      },
      "outputs": [],
      "source": [
        "#venues\n",
        "for idx, row in df_graph_publications.iterrows():\n",
        "    local_id= \"venue-\" + str(row[\"id\"])\n",
        "    subj = URIRef(base_url + local_id)\n",
        "\n",
        "    if row[\"venue_type\"] == \"journal\":\n",
        "        my_graph.add((subj, RDF.type, Journal))\n",
        "\n",
        "    if row[\"venue_type\"] == \"book\":\n",
        "        my_graph.add((subj, RDF.type, Book))\n",
        "\n",
        "    if row[\"venue_type\"] == \"proceedings\":\n",
        "        my_graph.add((subj, RDF.type, Proceedings))\n",
        "        my_graph.add((subj, event, Literal(row[\"event\"])))\n",
        "\n",
        "    my_graph.add((subj, title, Literal(row[\"publication_venue\"])))\n",
        "    my_graph.add((subj, publisher, URIRef(base_url + \"publisher-\" + str(row[\"publisher\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbf17f9-aa84-4b32-a9cb-5e45c01cd818",
      "metadata": {
        "id": "5cbf17f9-aa84-4b32-a9cb-5e45c01cd818"
      },
      "source": [
        "The same approach has been used to add information about the publications, as shown as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01c04ca7-cfcd-4f60-8a9a-3f4116b60557",
      "metadata": {
        "id": "01c04ca7-cfcd-4f60-8a9a-3f4116b60557"
      },
      "outputs": [],
      "source": [
        "#publications\n",
        "            \n",
        "for idx, row in df_graph_publications.iterrows():\n",
        "    local_id= \"publication-\" + str(row[\"id\"])\n",
        "\n",
        "    subj= URIRef(base_url + local_id)\n",
        "\n",
        "    if row[\"type\"] == \"journal-article\":\n",
        "        my_graph.add((subj, RDF.type, JournalArticle))\n",
        "        my_graph.add((subj, issue, Literal(row[\"issue\"])))\n",
        "        my_graph.add((subj, volume, Literal(row[\"volume\"])))\n",
        "\n",
        "    if row[\"type\"] == \"book-chapter\":\n",
        "        my_graph.add((subj, RDF.type, BookChapter))\n",
        "        my_graph.add((subj, chapterNumber, Literal(row[\"chapter\"])))\n",
        "\n",
        "    if row[\"type\"] == \"proceedings-paper\":\n",
        "        my_graph.add((subj, RDF.type, ProceedingsPaper))\n",
        "\n",
        "\n",
        "    my_graph.add((subj, title, Literal(str(row[\"title\"]))))\n",
        "    my_graph.add((subj, id, Literal(row[\"id\"])))\n",
        "    my_graph.add((subj, publicationYear, Literal(str(row[\"publication_year\"]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3377973-0e57-45b5-b5d9-e1d3f90ace21",
      "metadata": {
        "id": "c3377973-0e57-45b5-b5d9-e1d3f90ace21"
      },
      "source": [
        "In this way we added to our RDF graph the data that we could obtain from the CSV file. </br>\n",
        "The difficult part has been that of connecting this information with all the other data contained in the JSON file. We managed this task using the `local_id`, that we created (for each publication), by adding to the string \"publication-\" the value contained in the column \"id\" of the `DataFrame`, that is the doi, a unique value for each publication."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d5a79b-4631-48f9-855e-d8fa663e093a",
      "metadata": {
        "id": "c4d5a79b-4631-48f9-855e-d8fa663e093a"
      },
      "source": [
        "In order to upload the data contained in the JSON file, we had first to use specific functions of the Python package `json` to load the JSON document in Python. In particular, we used the function `load` to import in Python a JSON object, that must be imported from the json package as usual. In this way in our `graph_data` variable we obtained a list of dictionaries, that has been created to map the JSON array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a01f3a3-d4c5-4df9-808b-fe5128a30461",
      "metadata": {
        "id": "4a01f3a3-d4c5-4df9-808b-fe5128a30461"
      },
      "outputs": [],
      "source": [
        "def uploadData (path):\n",
        "    if path.endswith(\".json\"):\n",
        "        from json import load \n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            graph_data = load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d73e4fe-1486-415a-b6b2-71b063e6d85a",
      "metadata": {
        "id": "0d73e4fe-1486-415a-b6b2-71b063e6d85a"
      },
      "source": [
        "Then we have created some dataframes starting from the key-value pairs of the dictionaries contained in the list accesible with the variable `graph_data`. We have chosen this way for accessing the information we needed for creating the triple statements. For example, starting from the key `authors`, we have created a `DataFrame` with the information about the authors (<b>givenName</b>, <b>familyName</b>, <b>orcid</b>), `df_author`, with the aim of creating the statements having the authors as subject. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03774d25-bdd5-4e09-b9d9-a3c584030669",
      "metadata": {
        "id": "03774d25-bdd5-4e09-b9d9-a3c584030669"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "#authors' dataframe\n",
        "\n",
        "findex=0\n",
        "for x in graph_data[\"authors\"]:\n",
        "    for y in graph_data[\"authors\"][x]:\n",
        "        findex+=1\n",
        "\n",
        "df_author=pd.DataFrame(columns=[\"family\",\"given\",\"orcid\"],index=range(findex))#dataframe with authors information\n",
        "ind=0\n",
        "for x in graph_data[\"authors\"]:\n",
        "    for y in graph_data[\"authors\"][x]:\n",
        "        df_author.iloc[ind] = (y[\"family\"],y[\"given\"],y[\"orcid\"])\n",
        "        ind+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c599e7c3-452a-4656-a8fe-850c67ea605a",
      "metadata": {
        "id": "c599e7c3-452a-4656-a8fe-850c67ea605a"
      },
      "source": [
        "So we have iterated on its rows (one row for each author) for creating our `local_id` for the authors.<br>\n",
        "We have also created a dictionary for storing all the URIs as values of keys containing the `orcid`, in order to use them in the relations not having the authors as subject and in which the class `author` is the target one.<br>\n",
        "And within this iteration, we have added the statements as we did for the data coming from the CSV document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2c18e7-dd1a-4ba8-9568-d0040d35b54e",
      "metadata": {
        "id": "5e2c18e7-dd1a-4ba8-9568-d0040d35b54e"
      },
      "outputs": [],
      "source": [
        "author_internal_id = {}\n",
        "for idx, row in df_author.iterrows():\n",
        "    local_id =\"author-\" + str(row[\"orcid\"])\n",
        "\n",
        "    subj=URIRef (base_url + local_id)\n",
        "    \n",
        "    # We put the new author resources created here, to use them\n",
        "    # when creating publications\n",
        "    author_internal_id[row[\"orcid\"]] = subj\n",
        "    author_fullname=(str(row[\"given\"])+ \"\"+ str(row[\"family\"]))\n",
        "\n",
        "    my_graph.add((subj, RDF.type, Person))\n",
        "    my_graph.add((subj, givenName, Literal(row[\"given\"])))\n",
        "    my_graph.add((subj, familyName, Literal(row[\"family\"])))\n",
        "    my_graph.add((subj, name, Literal(author_fullname)))\n",
        "    my_graph.add((subj, id, Literal(row[\"orcid\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a2198e-aa53-48b3-9f8f-d87cc438d505",
      "metadata": {
        "id": "e0a2198e-aa53-48b3-9f8f-d87cc438d505"
      },
      "source": [
        "We have also created another dataframe, `df_pub_author`, with the `orcid` of the authors and the `doi` of the publications, and we have used it later, together with the dictionary `author_internal_id`, in order to access to the URIs to assign them as object of the <b>relation between Publications and Authors</b>, as shown here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e7cdae-2375-48b5-bcaa-8e804601df88",
      "metadata": {
        "id": "33e7cdae-2375-48b5-bcaa-8e804601df88"
      },
      "outputs": [],
      "source": [
        "#dataframe with id publications and authors' orcid \n",
        "df_pub_author = pd.DataFrame (columns=[\"id\", \"orcid\"], index=range(findex)) #dataframe with doi and orcid of the authors\n",
        "ind =0\n",
        "for x in graph_data[\"authors\"]:\n",
        "    for y in graph_data[\"authors\"][x]:\n",
        "        df_pub_author.iloc[ind] = (x,y[\"orcid\"])\n",
        "        ind+=1\n",
        "\n",
        "#publications dataframe\n",
        "findex=0\n",
        "for x in graph_data[\"authors\"]:\n",
        "    findex += 1\n",
        "df_doi = pd.DataFrame (columns= [\"id\"], index = range(findex))\n",
        "ind = 0\n",
        "for x in graph_data[\"authors\"]:\n",
        "    df_doi.iloc[ind] = (x)\n",
        "    ind +=1\n",
        "    \n",
        "#statements with publications as subject\n",
        "\n",
        "for idx, row in df_doi.iterrows():\n",
        "    local_id = \"publication-\" + str(row[\"id\"])\n",
        "    subj = URIRef (base_url + local_id)\n",
        "\n",
        "    #predicate author\n",
        "    for ind, ref in df_pub_author.iterrows():\n",
        "        if ref[\"id\"] == row[\"id\"]:\n",
        "            my_graph.add((subj, author, author_internal_id[ref[\"orcid\"]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7025f3f-df63-4c3d-a61f-1964d7db1d31",
      "metadata": {
        "id": "f7025f3f-df63-4c3d-a61f-1964d7db1d31"
      },
      "source": [
        "In the code above it is possible to see how we handled the <b>statements with Publications as subjects</b> coming from the JSON file. We used as starting point the doi contained as keys in the authors' dictionary stored in the variable `graph_data` and we created a `DataFrame` with just the publications' doi, called `df_doi`. Iterating over this dataframe we created the `local_id` of our resources as said before, so adding to the string \"publication-\" the value contained in the column \"id\" of the DataFrame, that is the doi, a unique value for each publication. In this way we succeeded in linking data about publications coming from the CSV document with the ones coming from the JSON. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cc2b26d-cfde-4459-ba06-3b1535c7feff",
      "metadata": {
        "id": "2cc2b26d-cfde-4459-ba06-3b1535c7feff"
      },
      "source": [
        "We used the same approach for all the other classes. So we created:\n",
        "* a dataframe with publishers' information;\n",
        "* a dataframe with publications' doi and venues' identifiers;\n",
        "* a dataframe with publications' doi and references' doi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85274bc6-3a26-4ae3-9a2c-66db88e89b95",
      "metadata": {
        "id": "85274bc6-3a26-4ae3-9a2c-66db88e89b95"
      },
      "outputs": [],
      "source": [
        "#dataframe for publishers \n",
        "findex = 0\n",
        "for x in graph_data[\"publishers\"]:\n",
        "    findex+=1\n",
        "df_publishers = pd.DataFrame(columns=[\"id\", \"name\"], index=range(findex))\n",
        "ind=0 \n",
        "for x in graph_data[\"publishers\"]:\n",
        "    df_publishers.iloc[ind] = (graph_data[\"publishers\"][x][\"id\"], graph_data[\"publishers\"][x][\"name\"])\n",
        "    ind +=1\n",
        "#statements with publishers as subject\n",
        "for idx, row in df_publishers.iterrows():\n",
        "    local_id = \"publisher-\" + str(row[\"id\"])\n",
        "\n",
        "    subj= URIRef (base_url + local_id)\n",
        "\n",
        "    my_graph.add((subj, RDF.type, Organization))\n",
        "    my_graph.add((subj, name, Literal(row[\"name\"])))\n",
        "    my_graph.add((subj, id, Literal(row[\"id\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54cbfd63-fef8-4875-95de-c8e4553c6879",
      "metadata": {
        "id": "54cbfd63-fef8-4875-95de-c8e4553c6879"
      },
      "source": [
        "We managed the link between the class `Venue` and the class `Organization` again with the `local_id`, that this time we created using the string \"publisher-\" and the identifier of each organization, that is both in the JSON and in the CSV (in the column `\"publisher\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4827d943-c47e-4eb4-8137-71f3fa3af91f",
      "metadata": {
        "id": "4827d943-c47e-4eb4-8137-71f3fa3af91f"
      },
      "outputs": [],
      "source": [
        "#dataframe with publications'doi and venues id\n",
        "findex = 0\n",
        "for x in graph_data[\"venues_id\"]:\n",
        "    findex += 1\n",
        "df_pub_venues = pd.DataFrame(columns = [\"doi\",\"venues_id\"], index= range(findex))\n",
        "ind =0\n",
        "for x in graph_data[\"venues_id\"]:\n",
        "    df_pub_venues.iloc[ind] = (x, graph_data[\"venues_id\"][x])\n",
        "    ind +=1\n",
        "\n",
        "#statements with venues as subject\n",
        "venue_internal_id = dict()\n",
        "for idx, row in df_pub_venues.iterrows():\n",
        "\n",
        "    local_id = \"venue-\" + str(row[\"doi\"])\n",
        "    subj = URIRef (base_url + local_id)\n",
        "    \n",
        "    # We put the new venues resources created here, to use them\n",
        "    # when creating the relation between publications and venues \n",
        "    venue_internal_id[str(row[\"venues_id\"])] = subj\n",
        "\n",
        "    my_graph.add((subj, id, Literal(row[\"venues_id\"])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba0af71-fe2a-4eaf-ac64-e2a45573663a",
      "metadata": {
        "id": "aba0af71-fe2a-4eaf-ac64-e2a45573663a"
      },
      "source": [
        "We have used the dictionary `venue_internal_id`, in order to access to the URIs to assign them as object of the <b>relation between Publications and Venues</b>, as shown here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bab2a3-7e90-4455-b505-cfddbaf99b98",
      "metadata": {
        "id": "24bab2a3-7e90-4455-b505-cfddbaf99b98"
      },
      "outputs": [],
      "source": [
        "for idx, row in df_doi.iterrows():\n",
        "    local_id = \"publication-\" + str(row[\"id\"])\n",
        "    subj = URIRef (base_url + local_id)\n",
        "\n",
        "    #predicate isPartOf\n",
        "    for ind, ref in df_pub_venues.iterrows():\n",
        "        if ref[\"doi\"] == row[\"id\"]:\n",
        "            my_graph.add((subj, publicationVenue, venue_internal_id[str(ref[\"venues_id\"])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8370d98e-03fd-4113-b768-cac096c42ae1",
      "metadata": {
        "id": "8370d98e-03fd-4113-b768-cac096c42ae1"
      },
      "outputs": [],
      "source": [
        "#references \n",
        "findex=0\n",
        "for x in graph_data[\"references\"]:\n",
        "    for y in graph_data[\"references\"][x]:\n",
        "        findex+=1\n",
        "df_references = pd.DataFrame(columns=[\"id\", \"id_references\"], index=range(findex))\n",
        "ind = 0\n",
        "for x in graph_data[\"references\"]:\n",
        "    for y in graph_data[\"references\"][x]:\n",
        "        df_references.iloc[ind]= (x, y)\n",
        "        ind += 1\n",
        "\n",
        "publication_internal_id= {}\n",
        "            \n",
        "for idx, row in df_doi.iterrows():\n",
        "\n",
        "    local_id = \"publication-\" + str(row[\"id\"])\n",
        "    subj = URIRef (base_url + local_id)\n",
        "    \n",
        "    #we put the resources just created in this dictonary to use \n",
        "    #them in the statements with predicate \"cites\"\n",
        "    publication_internal_id[row[\"id\"]] = subj\n",
        "    \n",
        "for idx, row in df_doi.iterrows():\n",
        "    local_id = \"publication-\" + str(row[\"id\"])\n",
        "    subj = URIRef (base_url + local_id)\n",
        "\n",
        "    #predicate cites\n",
        "    for ind, ref in df_references.iterrows():\n",
        "        if ref[\"id\"] == row[\"id\"]:\n",
        "            if ref[\"id_references\"] in publication_internal_id:\n",
        "                my_graph.add((subj, cites, publication_internal_id[ref[\"id_references\"]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd44662d-9e41-4717-99f4-91cb7b1055ec",
      "metadata": {
        "id": "fd44662d-9e41-4717-99f4-91cb7b1055ec"
      },
      "source": [
        "References are publications so in this case we stored the publications in a dictionary, using as keys the publications' doi, and we used the values as objects in the statements with the predicate \"schema:citation\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04250173-a3a5-466b-b713-3c4b8060ebbd",
      "metadata": {
        "id": "04250173-a3a5-466b-b713-3c4b8060ebbd"
      },
      "source": [
        "Some clarifications: \n",
        "* we created two other dataframes, `df_all_authors_id` and `df_all_references`, containing respectively the doi of the publication and the orcid of all the authors who wrote that publication, and the doi of the publication and the doi of the publications cited by the publication that we are considering. We used these two dataframes to add, through new statements (one with as subject the class Publication and as predicate \"schema:creator\", and the other with as subject the class `Publication` and as predicate \"schema:relatedLink\"), all the authors of a publication and all the publications cited by one publication in a single cell.\n",
        "* we created a dictionary with publications' doi as keys and number of times a publication is cited as values and we used these values as objects in the statements having the class  `Publication ` as subject and schema:isCitedBy as predicate. We added this information in our graph database in order to use it with some methods of the `TriplestoreQueryProcessor `, in particular  `getMostCitedPublication ` and  `getMostCitedVenue `."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80368189",
      "metadata": {
        "id": "80368189"
      },
      "source": [
        "## The Triplestore Query Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e55bbb7",
      "metadata": {
        "id": "1e55bbb7"
      },
      "source": [
        "Then, we created the `TriplestoreQueryProcessor`, which according to the data model is a subclass of the `TriplestoreProcessor` and inherits its attribute `endpointUrl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e65c97",
      "metadata": {
        "id": "63e65c97"
      },
      "outputs": [],
      "source": [
        "class TriplestoreQueryProcessor (TriplestoreProcessor):\n",
        "    def __init__(self):\n",
        "        super().__init__(self.endpointUrl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8a27ec2",
      "metadata": {
        "id": "e8a27ec2"
      },
      "source": [
        "We created all the methods contained in the data model for this class, adding in the parameters for each method the input value to be used for the query (for example, in `getPublicationsPublishedInYear`, we put `year` as a parameter), defining them in this way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8921f17",
      "metadata": {
        "id": "b8921f17"
      },
      "outputs": [],
      "source": [
        "def getPublicationsPublishedInYear (self, year):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f26648e9",
      "metadata": {
        "id": "f26648e9"
      },
      "source": [
        "After that, we defined a variable for the query corresponding to the method. We have stored each query as a list of strings. This was done in order to insert within the SPARQL query string the variable corresponding to the input value used for searching information in the database. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f72060f",
      "metadata": {
        "id": "1f72060f"
      },
      "outputs": [],
      "source": [
        "    query_1= [ \"\"\"\n",
        "            PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "            PREFIX schema: <https://schema.org/>\n",
        "\n",
        "            SELECT ?internalID ?id ?type ?title ?chapterNumber ?issue ?volume ?publicationVenue ?author ?cites\n",
        "            WHERE {\n",
        "                VALUES ?type {\n",
        "                    schema:ScholarlyArticle schema:Chapter schema:Article}\n",
        "                ?internalID rdf:type ?type .\n",
        "                ?internalID schema:identifier ?id.\n",
        "                ?internalID schema:name ?title.\n",
        "                ?internalID schema:creator ?author.\n",
        "                ?internalID schema:datePublished\"\"\", str(\"'\"+year+\"'\"), \".\", \n",
        "                \"\"\"?internalID schema:isPartOf ?publicationVenue.\n",
        "                                    \n",
        "            \n",
        "                OPTIONAL { ?internalID schema:Number ?chapterNumber .}\n",
        "                OPTIONAL {?internalID schema:issueNumber ?issue .}\n",
        "                OPTIONAL {?internalID schema:volumeNumber ?volume .}\n",
        "                OPTIONAL {?internalID schema:relatedLink ?cites.}\n",
        "                \n",
        "            }\n",
        "            \"\"\"\n",
        "                ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28a8776",
      "metadata": {
        "id": "a28a8776"
      },
      "source": [
        "The queries are written in the SPARQLQuery language for graph databases and follow its syntax. They are composed by:\n",
        " * the prefixes, indicating the URLs of the schemas we have used for getting our properties (RDF and Schema.org);\n",
        " * the `SELECT` part, in which we selected all the variables we need in the result of the query:\n",
        " * the `WHERE` close, which contains the statements to be satisfied in the query;\n",
        " * eventually,some `OPTIONAL` statements that don't have to be necessarily satisfied.<br>\n",
        " \n",
        " For realizing some of the queries we have used also other `SPARQL` functions(?).<br>\n",
        " For example, we have used the `FILTER` together with `REGEX` when we needed to check if the input strings was contained in the data of our database. This happened, for example, in the `getPublicationsByAuthorsName` query. For creating this specific query, we have added a triple statement in the graph database. This statement has the author resource as `subject`, the property `name` from Schema.org as a `predicate` and a Literal with the author's fullname as `object`.\n",
        " Thanks to this statement we were able to check if th input string was contained in either in the given or in the family name(or in both) of a specific author, in this way: \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b89702",
      "metadata": {
        "id": "f5b89702"
      },
      "outputs": [],
      "source": [
        "    def getPublicationsByAuthorName(self, inputstring):\n",
        "            \n",
        "            query_10=[\"\"\"\n",
        "    PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
        "    PREFIX schema: <https://schema.org/>\n",
        "    SELECT ?internalID ?title ?doi ?publicationYear ?venueID ?type ?issue ?volume ?chapterNumber ?all_authors_id ?id_all_references\n",
        "        WHERE{ \n",
        "        ?internalID schema:author ?author;\n",
        "                    schema:name ?title;\n",
        "                    schema:datePublished ?publicationYear;\n",
        "                    schema:identifier ?doi;\n",
        "                    rdf:type ?type.\n",
        "        OPTIONAL{?internalID schema:isPartOf ?publicationVenue.\n",
        "        ?publicationVenue schema:identifier ?venueID.}\n",
        "                                \n",
        "        \n",
        "        ?author  schema:name ?fullname.\n",
        "        ?author  schema:identifier ?authorID.\n",
        "        \n",
        "        ?internalID schema:creator ?all_authors_id.         \n",
        "        FILTER REGEX(?fullname,\"\"\",str(\"'\"+inputstring+\"'\"), \"\"\", \"i\").\n",
        "            \n",
        "        OPTIONAL {?internalID schema:issueNumber ?issue.\n",
        "                ?internalID schema:volumeNumber ?volume.\n",
        "                }\n",
        "        OPTIONAL {?internalID schema:Number ?chapterNumber.}\n",
        "        OPTIONAL {?internalID schema:relatedLink ?id_all_references.}\n",
        "        }\n",
        "    \"\"\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b135b84",
      "metadata": {
        "id": "7b135b84"
      },
      "source": [
        "After having written the queries in this way, we needed to join all the strings composing each single query in a unique string, in order to use it in the following step. For doing that, we used the `.join` method, which allowed to transform the list of strings into one single string, in this way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5042c8e",
      "metadata": {
        "id": "b5042c8e"
      },
      "outputs": [],
      "source": [
        "    stringa=(\" \".join(query_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f854380",
      "metadata": {
        "id": "8f854380"
      },
      "source": [
        "At this point, we had to obtain a DataFrame as the final result of every qwery. Therefore, we have used the funcion `get`imported from `sparql_dataframe`. This funcion, taking in input the endpoint URL of the database and the string representing the query to do in the database, as well as the boolean `True`, allowed us obtaining a dataframes as a result from every query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42c71779",
      "metadata": {
        "id": "42c71779"
      },
      "outputs": [],
      "source": [
        "    from sparql_dataframe import get\n",
        "    \n",
        "    df_final = get (TriplestoreProcessor.getEndpointUrl(TriplestoreProcessor), stringa, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f78da77",
      "metadata": {
        "id": "5f78da77"
      },
      "source": [
        "Then, we have modified every dataframe setting the type of the columns as `string` and remuving the final \".0\" from  string values still containing it. Indeed, launching the queries, we noticed that some columns of the final dataframes had not the type `string` even if we had set it in the initial dataframe created while opening the CSV. We have also tried to add them as string Literals in the statement of the graph, but the problem was still present. Therefore we have used a function in each method for removing the \".0\" from the columns of the dataframe resulting from the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb81f9eb",
      "metadata": {
        "id": "eb81f9eb"
      },
      "outputs": [],
      "source": [
        "     \n",
        "    def remove_dotzero(s):\n",
        "        return s.replace(\".0\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35a5fdf",
      "metadata": {
        "id": "b35a5fdf"
      },
      "source": [
        "# The Generic Query Processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f728fe06",
      "metadata": {
        "id": "f728fe06"
      },
      "source": [
        "In creating the Generic query processor, we have faced some issues. The first thing we did was defining the class.\n",
        "We have also defined here the variable `queryProcessor` , which is the list containing the two query processors to use for the generic queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5072955b",
      "metadata": {
        "id": "5072955b"
      },
      "outputs": [],
      "source": [
        "class GenericQueryProcessor(object):\n",
        "    def __init__(self, queryProcessor):\n",
        "        self.queryProcessor=queryProcessor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763b8f68",
      "metadata": {
        "id": "763b8f68"
      },
      "source": [
        "The first two methods we defined are  `addQueryProcessor`, meant to add query processors to the `queryProcessor` list, and `cleanQueryProcessor`, meant to remove from the same list the query processors we want to use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f65ffab",
      "metadata": {
        "id": "6f65ffab"
      },
      "outputs": [],
      "source": [
        "def cleanQueryProcessors(self):\n",
        "        for obj in self.queryProcessor:\n",
        "            self.queryProcessor.remove(obj)\n",
        "            return self.queryProcessor\n",
        "            \n",
        "def addQueryProcessor(self, input):\n",
        "        self.queryProcessor.append(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61787cb2",
      "metadata": {
        "id": "61787cb2"
      },
      "source": [
        "Then, for the methods involving the execution of the queries, we have proceeded thid way:\n",
        "* we have defined the method using the input as a parameter;\n",
        "* we have created an empty dataframe with the aim of concatenating it to the dataframes resulting from the separated queries;\n",
        "* then, supposing that we have the two processors in the list `queryProcessor`, we have iterated on that list for executing the specific query of the method on every processor;\n",
        "* for every query executed this way, we have used the `concat` function of `pandas` for joining the resulting dataframe with the `result` dataframe, initially set as empty.\n",
        "\n",
        "After having obtained a unique dataframe with the results of the execution of the query in every processor added in the list, we have applied some modifications to this dataframe: we have removed the the `NaN` from the empty fields and we have dropped the duplicates. We have proceeded removing the duplicates because we wanted to create, starting from this dataframe, a single object for each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce761a44",
      "metadata": {
        "id": "ce761a44"
      },
      "outputs": [],
      "source": [
        "def getPublicationsPublishedInYear(self, inputYear):\n",
        "        result = DataFrame()\n",
        "        self.finalresultlist=[]\n",
        "        for processor in self.queryProcessor: \n",
        "            q=processor.getPublicationsPublishedInYear(inputYear)\n",
        "            result = concat([result, q],ignore_index=True)\n",
        "        #removing the NaN\n",
        "        result= result.fillna(\"\")\n",
        "        result.drop_duplicates(subset=\"doi\", keep = \"first\", inplace=True)\n",
        "                    \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77cfa41e",
      "metadata": {
        "id": "77cfa41e"
      },
      "source": [
        "Then, we went on with the instantiation of the objects as requested for the implementation of these methods. \n",
        "Our aim was to use each row for instantiating an object of the corresponding class in the data model.\n",
        "For doing it, we have iterated on the row of the `result` dataframe, in order to get from its columns the constructors needed for the instantiation. <br>\n",
        "\n",
        "In this step, we had to face a problem: in the case of some parameters, like `author` for the class `Publication`, we had more than one corresponding value (e.g. more than one author for a publication). How to instanciate a `Publication` containing all of them in the values of the parameter `author`? <br>\n",
        "At first, we tried creating a list containing all the unique identifiers for the authors of the same publications, with the aim of using this list as a value for the parameter author, but we had problems in implementing this idea.<br>\n",
        "Therefore, we thought that a solution could be grouping all the identifiers of the authors of a single publication into one single value and using it for instantiating the `Publication` objects. <br>\n",
        "\n",
        "This implied the modification of our databases: we have added a statement in the graph database containing as object all the identifiers for the authors of a publication and a dataframe in the relational database containing, the same way, all the identifiers for all the authors of every publication.<br>\n",
        "This allowed us to select these values in the queries and use them for instantiating the objects of that class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3603de",
      "metadata": {
        "id": "cf3603de"
      },
      "outputs": [],
      "source": [
        "        for row_idx,row in result.iterrows():\n",
        "                    x=Publication(identifiers=row[\"doi\"],title=row[\"title\"],publicationYear=row[\"publicationYear\"],\n",
        "                    publicationVenue=row[\"venueID\"], author = row[\"all_authors_id\"],\n",
        "                                citedpublication = row[\"id_all_references\"])  \n",
        "\n",
        "                    self.finalresultlist.append(x)\n",
        "                    \n",
        "        return self.finalresultlist"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "graph_database_notebook(1).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "toc-showtags": false
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
